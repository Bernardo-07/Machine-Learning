WEEK 1

LESSON 2
Machine learning algorithms:
- supervised learning (Course 1 and Course 2)
- unsupervised learning (Course 3)
- recommender systems
- reinforcement learning
some applications: spam filtering, speech recognition, machine translation, online advertising, self-driving car, visual inspection*
supervised learning -> learns from being given "right answers" by regression (predict a number, infinitely many possible otputs) or classification (predict categories, small number of possible outputs)

LESSON 3
Linear regression
(x, y) = (input, output) variables
m -> number of training examples
feature variables -> model function -> prediction (estimated y)
model function: f(x) = wx + b (linear example)
our objective is to find a w and b that results in a estimated y close to y for all (x, y)
Cost function: J(w,b) = (1/2m)*(Soma de 1->m de (ý - y)²)
ý is a f(x)
the goal is minimize the cost function

LESSON 4
gradient descent algorithm: 
tmp_w = w - @*dJ(w,b)/dw, wich alfa(@) is called the learning rate (0 < @ < 1)
tmp_b = b - @*dJ(w,b)/db
w = tmp_w
b = tmp_b


WEEK 2

LESSON 1
multiple linear regression = using vectors
example:
w = np.array([10, 20, -30])
b = 40
x = np.array([1, 2, 3])
without vectorization:
f=0
for j in range(0, n):
	f = f + w[j] * x[j]
f = f + b
with vectorization:
f = np.dot(w, x) + b

LESSON 2
Rescaled the features = speed up gradient descent to find the blobal minimum
feature scaling, ex:
300 < a < 2000 and 0 < b < 5
ascaled = a/2000(max) and bscaled = b/5(max)
0.15 < ascaled < 1 and 0 < bscaled < 1
mean normalization?
z-score normalization?


WEEK 3

LESSON 1
logistic regression (sigmoid function)
returns the probability that y is equal to 1 given x and with parameters W and B
decision boundary when z = w*x +b = 0 k